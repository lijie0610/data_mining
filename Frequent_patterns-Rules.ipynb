{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仓库地址：https://github.com/lijie0610/data_mining/tree/Frequent_patterns-%26-Rules\n",
    "\n",
    "# 数据集\n",
    "## 选择Oakland Crime Statistics 2011 to 2016数据集进行分析\n",
    "## 数据集查看\n",
    "以2016年为例，查看其属性值和前五列数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110828 entries, 0 to 110827\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   Agency                     110827 non-null  object \n",
      " 1   Create Time                110827 non-null  object \n",
      " 2   Location                   110828 non-null  object \n",
      " 3   Area Id                    110827 non-null  object \n",
      " 4   Beat                       110247 non-null  object \n",
      " 5   Priority                   110827 non-null  float64\n",
      " 6   Incident Type Id           110827 non-null  object \n",
      " 7   Incident Type Description  110827 non-null  object \n",
      " 8   Event Number               110827 non-null  object \n",
      " 9   Closed Time                110827 non-null  object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "dataset=pd.read_csv(r'D:\\Users\\Desktop\\data\\Oakland Crime Statistics 2011 to 2016\\records-for-2016.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agency</th>\n",
       "      <th>Create Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area Id</th>\n",
       "      <th>Beat</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Incident Type Id</th>\n",
       "      <th>Incident Type Description</th>\n",
       "      <th>Event Number</th>\n",
       "      <th>Closed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OP</td>\n",
       "      <td>2016-01-01T00:00:57.000</td>\n",
       "      <td>ST&amp;MARKET ST</td>\n",
       "      <td>P1</td>\n",
       "      <td>05X</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415GS</td>\n",
       "      <td>415 GUNSHOTS</td>\n",
       "      <td>LOP160101000003</td>\n",
       "      <td>2016-01-01T00:32:30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OP</td>\n",
       "      <td>2016-01-01T00:01:25.000</td>\n",
       "      <td>AV&amp;HAMILTON ST</td>\n",
       "      <td>P3</td>\n",
       "      <td>26Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415GS</td>\n",
       "      <td>415 GUNSHOTS</td>\n",
       "      <td>LOP160101000005</td>\n",
       "      <td>2016-01-01T00:48:23.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OP</td>\n",
       "      <td>2016-01-01T00:01:43.000</td>\n",
       "      <td>ST&amp;CHESTNUT ST</td>\n",
       "      <td>P1</td>\n",
       "      <td>02X</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415GS</td>\n",
       "      <td>415 GUNSHOTS</td>\n",
       "      <td>LOP160101000008</td>\n",
       "      <td>2016-01-01T00:21:24.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OP</td>\n",
       "      <td>2016-01-01T00:01:48.000</td>\n",
       "      <td>WALLACE ST</td>\n",
       "      <td>P2</td>\n",
       "      <td>18Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415GS</td>\n",
       "      <td>415 GUNSHOTS</td>\n",
       "      <td>LOP160101000007</td>\n",
       "      <td>2016-01-01T01:15:03.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OP</td>\n",
       "      <td>2016-01-01T00:02:05.000</td>\n",
       "      <td>90TH AV</td>\n",
       "      <td>P3</td>\n",
       "      <td>34X</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415GS</td>\n",
       "      <td>415 GUNSHOTS</td>\n",
       "      <td>LOP160101000009</td>\n",
       "      <td>2016-01-01T00:54:52.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Agency              Create Time             Location Area Id Beat  Priority  \\\n",
       "0     OP  2016-01-01T00:00:57.000     ST&MARKET ST          P1  05X       2.0   \n",
       "1     OP  2016-01-01T00:01:25.000   AV&HAMILTON ST          P3  26Y       2.0   \n",
       "2     OP  2016-01-01T00:01:43.000   ST&CHESTNUT ST          P1  02X       2.0   \n",
       "3     OP  2016-01-01T00:01:48.000       WALLACE ST          P2  18Y       2.0   \n",
       "4     OP  2016-01-01T00:02:05.000          90TH AV          P3  34X       2.0   \n",
       "\n",
       "  Incident Type Id Incident Type Description     Event Number  \\\n",
       "0            415GS              415 GUNSHOTS  LOP160101000003   \n",
       "1            415GS              415 GUNSHOTS  LOP160101000005   \n",
       "2            415GS              415 GUNSHOTS  LOP160101000008   \n",
       "3            415GS              415 GUNSHOTS  LOP160101000007   \n",
       "4            415GS              415 GUNSHOTS  LOP160101000009   \n",
       "\n",
       "               Closed Time  \n",
       "0  2016-01-01T00:32:30.000  \n",
       "1  2016-01-01T00:48:23.000  \n",
       "2  2016-01-01T00:21:24.000  \n",
       "3  2016-01-01T01:15:03.000  \n",
       "4  2016-01-01T00:54:52.000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "将数据转化为关联规则挖掘形式，删除缺失值。\n",
    "选择Location、Area id、Beat、Incident Type Id、Incident Type Description、Event Number属性进行分析。\n",
    "缺失值删除后由110828行变为110247行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 110247 entries, 0 to 110826\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Location          110247 non-null  object \n",
      " 1   Area Id           110247 non-null  object \n",
      " 2   Beat              110247 non-null  object \n",
      " 3   Priority          110247 non-null  float64\n",
      " 4   Incident Type Id  110247 non-null  object \n",
      " 5   Event Number      110247 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data=dataset[[\"Location\", \"Area Id\", \"Beat\", \"Priority\", \"Incident Type Id\", \"Event Number\"]]\n",
    "data=data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 频繁模式和关联规则挖掘\n",
    "## 找出频繁模式\n",
    "## 导出关联规则，计算其支持度和置信度;\n",
    "## 对规则进行评价，可使用Lift、卡方和其它教材中提及的指标, 至少2种；\n",
    "挖掘算法为Apriori 算法。\n",
    "算法步骤如下：\n",
    "找出出现频率最大的一个项L1。\n",
    "根据L1找频繁“2项集”的集合C2。\n",
    "并剪掉不满足支持度阈值的项，得到L2。\n",
    "根据L2找频繁“3项集”的集合C3。\n",
    "根据性质和支持度阈值进行剪枝，得到L3。\n",
    "循环上述过程，直到得到空集C，即直到不能发现更大的频集L。\n",
    "计算最大频集L的非空子集，两两计算置信度，得到大于置信度阈值的强关联规则。\n",
    "\n",
    "规则评价选择：Lift和Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori():\n",
    "    def __init__(self,min_support=0.1,min_conf=0.5):\n",
    "        self.min_support = min_support#支持度阈值\n",
    "        self.min_conf = min_conf#置信度阈值\n",
    "    \n",
    "    def candidate_datasets_l1(self, dataset):       #创建单项频繁项集及L1\n",
    "        l1 = []\n",
    "        progress = ProgressBar()\n",
    "        for data in progress(dataset):\n",
    "            for item in data:\n",
    "                if [item] not in l1:\n",
    "                    l1.append([item])\n",
    "        return [frozenset(item) for item in l1]\n",
    "    \n",
    "    def candidate_datasets_lk(self, dataset):             #根据L1创建多项频繁项集Lk。获取频繁模式。\n",
    "        l1 = self.candidate_datasets_l1(dataset)        \n",
    "        dataset = [set(data) for data in dataset]\n",
    "        frequent_patterns, support = self.lk_low_support_filtering(dataset, l1)\n",
    "        frequent_patterns = [frequent_patterns]\n",
    "        k = 2\n",
    "        while len(frequent_patterns[k-2]) > 0:\n",
    "            lk = self.apriori_gen(frequent_patterns[k-2], k)        \n",
    "            Fk, support_k = self.lk_low_support_filtering(dataset, lk)      \n",
    "            support.update(support_k)\n",
    "            frequent_patterns.append(Fk)\n",
    "            k += 1\n",
    "        return frequent_patterns, support\n",
    "\n",
    "\n",
    "    def lk_low_support_filtering(self, dataset, lk):   #选择符合支持度的频繁模式\n",
    "        lk_count = dict()\n",
    "        for data in dataset:\n",
    "            for cand in lk:\n",
    "                if cand.issubset(data):\n",
    "                    if cand not in lk_count:\n",
    "                        lk_count[cand] = 1\n",
    "                    else:\n",
    "                        lk_count[cand] += 1\n",
    "\n",
    "        num_items = float(len(dataset))\n",
    "        fp = []\n",
    "        support = dict()\n",
    "        for key in lk_count:\n",
    "            sup  = lk_count[key] / num_items\n",
    "            if sup >= self.min_support:\n",
    "                fp.insert(0, key)\n",
    "            support[key] = sup\n",
    "        return fp, support\n",
    "\n",
    "    def apriori_gen(self, Fk, k):       #合并时检测\n",
    "        lk = []\n",
    "        len_Fk = len(Fk)\n",
    "\n",
    "        for i in range(len_Fk):\n",
    "            for j in range(i+1, len_Fk):\n",
    "                F1 = list(Fk[i])[:k-2]\n",
    "                F2 = list(Fk[j])[:k-2]\n",
    "                F1.sort()\n",
    "                F2.sort()\n",
    "                if F1 == F2:\n",
    "                    lk.append(Fk[i] | Fk[j])\n",
    "        return lk\n",
    "\n",
    "    def change_data_dict(self,datasets):# 将数据转为数据字典\n",
    "        dataset = []\n",
    "        col = [\"Location\", \"Area Id\", \"Beat\", \"Priority\", \"Incident Type Id\", \"Event Number\"]\n",
    "        for data_line in datasets:\n",
    "            data = []\n",
    "            for i, value in enumerate(data_line):\n",
    "                if not value:\n",
    "                    data.append((col[i], 'NA'))\n",
    "                else:\n",
    "                    data.append((col[i], value))\n",
    "            dataset.append(data)\n",
    "        return dataset\n",
    "                \n",
    "    def get_rules(self, frequent_patterns, support):#获取关联规则，并计算支持度和置信度\n",
    "        rules = []\n",
    "        for i in range(1, len(frequent_patterns)):\n",
    "            for frequent_pattern in frequent_patterns[i]:\n",
    "                H1 = [frozenset([item]) for item in frequent_pattern]\n",
    "                if i > 1:\n",
    "                    self.create_rules(frequent_pattern, H1, support, rules)\n",
    "                else:\n",
    "                    self.conviction(frequent_pattern, H1, support, rules)\n",
    "        return rules\n",
    "\n",
    "    def create_rules(self, frequent_pattern, H, support, rules):#迭代产生关联规则\n",
    "        m = len(H[0])\n",
    "        if len(frequent_pattern) > (m+1):\n",
    "            Hmp1 = self.apriori_gen(H, m+1)\n",
    "            Hmp1 = self.conviction(frequent_pattern, Hmp1, support, rules)\n",
    "            if len(Hmp1) > 1:\n",
    "                self.create_rules(frequent_pattern, Hmp1, support, rules)\n",
    "\n",
    "    def conviction(self, frequent_pattern, H, support, rules):          #置信度计算，并对关联规则进行评价\n",
    "        prunedH = []\n",
    "        for reasoned_item in H:\n",
    "            sup = support[frequent_pattern]#支持度\n",
    "            conf = sup / support[frequent_pattern - reasoned_item]#置信度\n",
    "            lift = conf / support[reasoned_item]#lift指标\n",
    "            jaccard = sup / (support[frequent_pattern - reasoned_item] + support[reasoned_item] - sup)#jaccard指标\n",
    "            if conf >= self.min_conf:\n",
    "                rules.append((frequent_pattern-reasoned_item, reasoned_item, sup, conf, lift, jaccard))\n",
    "                prunedH.append(reasoned_item)\n",
    "        return prunedH\n",
    "    \n",
    "    def show_frequent_patterns(self,support):#频繁模式可视化\n",
    "        for (key, value) in support:\n",
    "            result_dict = {'frequent_pattern': None, 'support': None}\n",
    "            set_result = list(key)\n",
    "            sup_result = value\n",
    "            if sup_result < self.min_support:\n",
    "                continue\n",
    "            result_dict['frequent_pattern'] = set_result\n",
    "            result_dict['support'] = sup_result\n",
    "            print(result_dict)\n",
    "            print('\\n')\n",
    "    \n",
    "    def show_rules(self,rules):#关联规则可视化\n",
    "        for result in rules:\n",
    "            result_dict = {'reason': None, 'result': None, 'support': None, 'conviction': None, 'lift': None, 'jaccard': None}\n",
    "            X_set, Y_set, support, conf, lift, jaccard = result\n",
    "            result_dict['reason'] = list(X_set)\n",
    "            result_dict['result'] = list(Y_set)\n",
    "            result_dict['support'] = support\n",
    "            result_dict['conviction'] = conf\n",
    "            result_dict['lift'] = lift\n",
    "            result_dict['jaccard'] = jaccard\n",
    "            print(result_dict)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from progressbar import *\n",
    "apriori = Apriori()\n",
    "datasets = data.values.tolist()\n",
    "dataset=apriori.change_data_dict(datasets)\n",
    "# 找出频繁模式\n",
    "frequent_patterns, support = apriori.candidate_datasets_lk(dataset)\n",
    "print(\"support \", support)\n",
    "# 导出关联规则，及其支持度和置信度\n",
    "rules = apriori.get_rules(frequent_patterns, support)\n",
    "rules = sorted(rules, key=lambda x: x[3], reverse=True)\n",
    "print(\"rules \", rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果可视化展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frequent_pattern': [('Priority', 2.0)], 'support': 0.7781073407893185}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P3')], 'support': 0.4289731239852332}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P1')], 'support': 0.37308951717507055}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P3'), ('Priority', 2.0)], 'support': 0.32334666702948833}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P1'), ('Priority', 2.0)], 'support': 0.2964615817210446}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Priority', 1.0)], 'support': 0.22189265921068146}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P2')], 'support': 0.17775540377515942}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P2'), ('Priority', 2.0)], 'support': 0.13877021596959555}\n",
      "\n",
      "\n",
      "{'frequent_pattern': [('Area Id', 'P3'), ('Priority', 1.0)], 'support': 0.10562645695574482}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#频繁模式可视化\n",
    "support = sorted(support.items(), key=lambda d: d[1], reverse=True)\n",
    "apriori.show_frequent_patterns(support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': [('Area Id', 'P1')], 'result': [('Priority', 2.0)], 'support': 0.2964615817210446, 'conviction': 0.7946124671788388, 'lift': 1.0212118888028705, 'jaccard': 0.34684608201035744}\n",
      "\n",
      "\n",
      "{'reason': [('Area Id', 'P2')], 'result': [('Priority', 2.0)], 'support': 0.13877021596959555, 'conviction': 0.7806807164361892, 'lift': 1.003307224481728, 'jaccard': 0.1698341511067694}\n",
      "\n",
      "\n",
      "{'reason': [('Area Id', 'P3')], 'result': [('Priority', 2.0)], 'support': 0.32334666702948833, 'conviction': 0.7537690567314401, 'lift': 0.9687211740822423, 'jaccard': 0.36588695357645057}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#关联规则可视化\n",
    "apriori.show_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据结果可知，我们可以挖掘地区和犯罪数量和案件严重性的关系。\n",
    "通过观察频繁模型可以看出，Area Id 为 p3 的支持度较高，该地区的犯罪案件数量最多。\n",
    "通过观察关联规则得知，Area Id：P1与Priority：2.0的置信度较高，说明该地区的案件较为严重。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
